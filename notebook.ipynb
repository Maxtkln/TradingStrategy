{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Data Exploration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 451113 entries, 0 to 451112\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   ticker  451113 non-null  object \n",
      " 1   date    451113 non-null  object \n",
      " 2   last    451113 non-null  float64\n",
      " 3   volume  451113 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 13.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "     ticker        date      last   volume\n",
       " 0  1332 JT  2013-01-04  169.0987  1464100\n",
       " 1  1332 JT  2013-01-07  166.3266  1783500\n",
       " 2  1332 JT  2013-01-08  166.3266  1759800\n",
       " 3  1332 JT  2013-01-09  165.4026   767800\n",
       " 4  1332 JT  2013-01-10  167.2507  1503100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "data_info = data.info()\n",
    "first_few_rows = data.head()\n",
    "\n",
    "data_info, first_few_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of asset : 248\n",
      "Number of dates : 2005\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of asset : {data[\"ticker\"].nunique()}')\n",
    "print(f'Number of dates : {data[\"date\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2013-01-04', '2013-01-07', '2013-01-08', ..., '2021-03-17',\n",
       "       '2021-03-18', '2021-03-19'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"date\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have daily data from 2013-01-04 to 2021-03-19 of 248 assets, we will first analyse the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.sort_values(by='date')\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "unique_tickers = data['ticker'].unique()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ticker in unique_tickers:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "    asset_data = data[data['ticker'] == ticker]\n",
    "    \n",
    "    axes[0].plot(asset_data['date'], asset_data['last'])\n",
    "    axes[0].set_title(f'{ticker} - Price')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Price')\n",
    "\n",
    "    axes[1].plot(asset_data['date'], asset_data['volume'])\n",
    "    axes[1].set_title(f'{ticker} - Volume')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Volume')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in unique_tickers:\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(15, 5))\n",
    "    asset_data = data[data['ticker'] == ticker]\n",
    "\n",
    "    returns = asset_data['last'].pct_change().dropna()\n",
    "    sns.histplot(returns, ax=ax, kde=True)\n",
    "    ax.set_title(f'{ticker} - Returns Distribution')\n",
    "    ax.set_xlabel('Returns')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker    0\n",
       "date      0\n",
       "last      0\n",
       "volume    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we can observe the following:\n",
    "\n",
    "The price and volume data vary significantly across different assets but there should be correlation.\n",
    "Our different assets have different lengths of data, we don't have from 2013 to 2021 for all assets.\n",
    "\n",
    "The distribution of returns is approximately centered around zero for the large majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = data.pivot(index='date', columns='ticker', values='last')\n",
    "price_correlation = price_data.corr()\n",
    "\n",
    "volume_data = data.pivot(index='date', columns='ticker', values='volume')\n",
    "volume_correlation = volume_data.corr()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Price Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(volume_correlation, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Volume Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prices let us suggest that there are clusters of different assets that behave the same way. The volume data give us the information that there are some assets that trade at high volumes at the same time but it is not clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try  to present two type of work. The first one is a long-term strategy using asset allocation and the second one is a daily trading strategy that use both price and volume information. We will develop simple strategies that can be improved in many ways and also be shaped using our own risk aversion, investment horizon and the quantity of money we want to invest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term allocation strategy \n",
    "\n",
    " We will use historical price data to allocate weights to each asset in the portfolio. The goal is to maximize the portfolio's return while minimizing its risk. : mean-variance optimization approach. We do the Markovitz Portafolio optimisation. Other methods such as **'Equally-weighted\n",
    "risk contributions portfolios'** can be used by changing the 'objective' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = data.pivot(index='date', columns='ticker', values='last').pct_change()\n",
    "\n",
    "avg_returns = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    "\n",
    "def objective(weights, avg_returns, cov_matrix, risk_aversion):\n",
    "    portfolio_return = np.dot(weights, avg_returns)\n",
    "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    return -portfolio_return + risk_aversion * portfolio_volatility\n",
    "\n",
    "num_assets = len(unique_tickers)\n",
    "initial_weights = np.ones(num_assets) / num_assets # Equally weighted portfolio\n",
    "\n",
    "constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}\n",
    "\n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "\n",
    "risk_aversion = 1 # We take 1 as a first test.\n",
    "optimal_weights = minimize(objective, initial_weights, args=(avg_returns, cov_matrix, risk_aversion),\n",
    "                           method='SLSQP', bounds=bounds, constraints=constraints)['x']\n",
    "\n",
    "optimal_return = np.dot(optimal_weights, avg_returns)\n",
    "optimal_volatility = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))\n",
    "\n",
    "portfolio_values = (returns * optimal_weights).sum(axis=1).cumsum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(portfolio_values.index, portfolio_values)\n",
    "ax.set_title('Portfolio Values Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Portfolio Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_weights, optimal_return, optimal_volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential improvement could be to include a risk-free rate in the optimization to calculate the Sharpe ratio and do a out-of-sample test to see if the strategy is robust using a risk avertion parameter tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading on the short-term : the use of signals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to plot some signals of our data to have a better idea. We will use the following signals:\n",
    "1. Moving Average: \n",
    "   $ \\text{SMA}(n) = \\frac{\\sum_{i=1}^n \\text{Price}_i}{n}$\n",
    "2. Relative Strength Index (RSI): \n",
    "   $ \\text{RSI} = 100 - \\left( \\frac{100}{1 + \\text{RS}} \\right) $\n",
    "   where $ \\text{RS} = \\frac{\\text{Average Gain}}{\\text{Average Loss}} $\n",
    "3. Exponential Moving Average : \n",
    "   $\\text{EMA}(n) = \\alpha \\cdot \\text{Price}_{n} + (1 - \\alpha) \\cdot \\text{EMA}_{n-1}$\n",
    "   where $\\alpha = \\frac{2}{n + 1}$ is the smoothing factor and $n$ is the length of the EMA.\n",
    "4. Moving Average Convergence Divergence (MACD): \n",
    "   $ \\text{MACD} = \\text{EMA}_{\\text{fast}} - \\text{EMA}_{\\text{slow}} $\n",
    "   and the signal line is a moving average of the MACD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['SMA_50'] = data.groupby('ticker')['last'].transform(lambda x: x.rolling(window=50).mean())\n",
    "data['SMA_200'] = data.groupby('ticker')['last'].transform(lambda x: x.rolling(window=200).mean())\n",
    "\n",
    "delta = data.groupby('ticker')['last'].transform(lambda x: x.diff())\n",
    "gain = (delta.where(delta > 0, 0)).groupby(data['ticker']).transform(lambda x: x.rolling(window=15).mean())\n",
    "loss = (-delta.where(delta < 0, 0)).groupby(data['ticker']).transform(lambda x: x.rolling(window=15).mean())\n",
    "rs = gain / loss\n",
    "data['RSI_15'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Calculate MACD\n",
    "data['EMA_12'] = data.groupby('ticker')['last'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "data['EMA_26'] = data.groupby('ticker')['last'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "data['MACD'] = data['EMA_12'] - data['EMA_26']\n",
    "data['MACD_signal'] = data.groupby('ticker')['MACD'].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "\n",
    "\n",
    "for sample_ticker in unique_tickers:\n",
    "    sample_data = data[data['ticker'] == sample_ticker]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(10, 15), sharex=True)\n",
    "\n",
    "    axes[0].plot(sample_data['date'], sample_data['last'], label='Price')\n",
    "    axes[0].plot(sample_data['date'], sample_data['SMA_50'], label='SMA 50')\n",
    "    axes[0].plot(sample_data['date'], sample_data['SMA_200'], label='SMA 200')\n",
    "    axes[0].set_title(f'{sample_ticker} - Price and Moving Averages')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(sample_data['date'], sample_data['EMA_12'], label='EMA 12')\n",
    "    axes[1].plot(sample_data['date'], sample_data['EMA_26'], label='EMA 26')\n",
    "    axes[1].set_title(f'{sample_ticker} - Exponential Moving Averages')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].plot(sample_data['date'], sample_data['RSI_15'], label='RSI 15')\n",
    "    axes[2].axhline(70, color='r', linestyle='--')\n",
    "    axes[2].axhline(30, color='g', linestyle='--')\n",
    "    axes[2].set_title(f'{sample_ticker} - RSI')\n",
    "    axes[2].legend()\n",
    "\n",
    "    axes[3].plot(sample_data['date'], sample_data['MACD'], label='MACD')\n",
    "    axes[3].plot(sample_data['date'], sample_data['MACD_signal'], label='Signal Line')\n",
    "    axes[3].set_title(f'{sample_ticker} - MACD')\n",
    "    axes[3].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signals with the volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Volume**:\n",
    "\n",
    "2. **On-Balance Volume (OBV)**:\n",
    "   $\n",
    "   \\text{OBV} = \\text{OBV}_{\\text{previous}} + \\begin{cases}\n",
    "     \\text{Volume} & \\text{if the price has increased} \\\\\n",
    "     -\\text{Volume} & \\text{if the price has decreased} \\\\\n",
    "     0 & \\text{if the price is constant}\n",
    "   \\end{cases}\n",
    "   $\n",
    "\n",
    "3. **Weighted On-Balance Volume (WOBV)**:\n",
    "   $\n",
    "   \\text{WOBV} = \\text{WOBV}_{\\text{previous}} + \\text{Volume} \\times \\Delta\\text{Price}\n",
    "   $\n",
    "\n",
    "4. **Percentage Change On-Balance Volume (PCOBV)**:\n",
    "   $\n",
    "   \\text{PCOBV} = \\text{PCOBV}_{\\text{previous}} + \\text{Volume} \\times \\frac{\\Delta\\text{Price}}{\\text{Previous Price}}\n",
    "   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['OBV'] = data.groupby('ticker')['volume'].transform(lambda x: (x * np.sign(x.diff())).cumsum())\n",
    "\n",
    "data['Delta_Price'] = data.groupby('ticker')['last'].transform(lambda x: x.diff())\n",
    "data['WOBV'] = data.groupby('ticker')['volume'].transform(lambda x: (x * data['Delta_Price']).cumsum())\n",
    "\n",
    "data['Pct_Change_Price'] = data.groupby('ticker')['last'].transform(lambda x: x.pct_change())\n",
    "data['PCOBV'] = data.groupby('ticker')['volume'].transform(lambda x: (x * data['Pct_Change_Price']).cumsum())\n",
    "\n",
    "for sample_ticker in unique_tickers:\n",
    "    sample_data = data[data['ticker'] == sample_ticker]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(10, 20), sharex=True)\n",
    "\n",
    "    axes[0].bar(sample_data['date'], sample_data['volume'], label='Volume')\n",
    "    axes[0].set_title(f'{sample_ticker} - Volume')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(sample_data['date'], sample_data['OBV'], label='OBV')\n",
    "    axes[1].set_title(f'{sample_ticker} - On-Balance Volume')\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(sample_data['date'], sample_data['WOBV'], label='WOBV')\n",
    "    axes[2].set_title(f'{sample_ticker} - Weighted On-Balance Volume')\n",
    "    axes[2].legend()\n",
    "\n",
    "    axes[3].plot(sample_data['date'], sample_data['PCOBV'], label='PCOBV')\n",
    "    axes[3].set_title(f'{sample_ticker} - Percentage Change On-Balance Volume')\n",
    "    axes[3].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too long to compute but we have an idea...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Confirmation Strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here is the explanation in English for the \"Trend Confirmation Strategy\" using price and volume signals:\n",
    "\n",
    "1. **Buy Signal:**\n",
    "   - Short-term moving average (SMA_50) crosses above long-term moving average (SMA_200).\n",
    "   - RSI is below 30 - classic oversold condition.\n",
    "   - MACD is above the signal line.\n",
    "   - Both OBV and WOBV are increasing and confirm the price trend.\n",
    "\n",
    "2. **Sell Signal:**\n",
    "   - Short-term moving average (SMA_50) crosses below long-term moving average (SMA_200).\n",
    "   - RSI is above 70 - classic overbought condition.\n",
    "   - MACD is below the signal line.\n",
    "   - Both OBV and WOBV are decreasing and confirm the price trend.\n",
    "\n",
    "This strategy combines the signals from both price and volume to make trading decisions. The buy and sell signals are generated based on the moving averages, RSI, and MACD for price, and OBV and WOBV for volume. The strategy is then backtested to calculate cumulative returns, and performance metrics such as total return, Sharpe ratio, and maximum drawdown are calculated to evaluate the strategy's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Signal'] = 0\n",
    "data['Position'] = 0\n",
    "\n",
    "# Buy signals\n",
    "data.loc[(data['SMA_50'] > data['SMA_200']) & (data['RSI_15'] < 30) & (data['MACD'] > data['MACD_signal']) & (data['OBV'] > data['OBV'].shift(1)) & (data['WOBV'] > data['WOBV'].shift(1)), 'Signal'] = 1\n",
    "\n",
    "# Sell signals\n",
    "data.loc[(data['SMA_50'] < data['SMA_200']) & (data['RSI_15'] > 70) & (data['MACD'] < data['MACD_signal']) & (data['OBV'] < data['OBV'].shift(1)) & (data['WOBV'] < data['WOBV'].shift(1)), 'Signal'] = -1\n",
    "\n",
    "data['Position'] = data['Signal'].replace(to_replace=0, method='ffill')\n",
    "data['Daily_Return'] = data.groupby('ticker')['last'].pct_change()\n",
    "data['Strategy_Return'] = data['Position'].shift(1) * data['Daily_Return']\n",
    "\n",
    "# Backtest the strategy\n",
    "strategy_returns = data.groupby('ticker')['Strategy_Return'].cumsum()\n",
    "\n",
    "for sample_ticker in unique_tickers:\n",
    "    sample_data = data[data['ticker'] == sample_ticker]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(sample_data['date'], sample_data['Strategy_Return'].cumsum())\n",
    "    ax.set_title(f'{sample_ticker} - Strategy Returns')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Cumulative Returns')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "total_return = data.groupby('ticker')['Strategy_Return'].sum()\n",
    "\n",
    "sharpe_ratio = data.groupby('ticker')['Strategy_Return'].mean() / data.groupby('ticker')['Strategy_Return'].std() * np.sqrt(252)\n",
    "\n",
    "drawdown = data.groupby('ticker').apply(lambda x: (x['Strategy_Return'].cumsum() - x['Strategy_Return'].cumsum().cummax()).min())\n",
    "\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Total Return': total_return,\n",
    "    'Sharpe Ratio': sharpe_ratio,\n",
    "    'Max Drawdown': drawdown\n",
    "})\n",
    "\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "axes[0].hist(performance_metrics['Total Return'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Total Returns')\n",
    "axes[0].set_xlabel('Total Return')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(performance_metrics['Sharpe Ratio'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Sharpe Ratios')\n",
    "axes[1].set_xlabel('Sharpe Ratio')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].hist(performance_metrics['Max Drawdown'], bins=30, color='salmon', edgecolor='black')\n",
    "axes[2].set_title('Distribution of Maximum Drawdowns')\n",
    "axes[2].set_xlabel('Max Drawdown')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.7716331178849"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_return.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Total Return</th>\n",
       "      <th>Worst Total Return</th>\n",
       "      <th>Best Sharpe Ratio</th>\n",
       "      <th>Worst Sharpe Ratio</th>\n",
       "      <th>Best Max Drawdown</th>\n",
       "      <th>Worst Max Drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <td>7951 JT</td>\n",
       "      <td>9766 JT</td>\n",
       "      <td>5019 JT</td>\n",
       "      <td>9434 JT</td>\n",
       "      <td>3864 JT</td>\n",
       "      <td>4506 JT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>2.757156</td>\n",
       "      <td>-0.991722</td>\n",
       "      <td>1.426093</td>\n",
       "      <td>-1.954547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.628885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Best Total Return Worst Total Return Best Sharpe Ratio  \\\n",
       "Ticker           7951 JT            9766 JT           5019 JT   \n",
       "Value           2.757156          -0.991722          1.426093   \n",
       "\n",
       "       Worst Sharpe Ratio Best Max Drawdown Worst Max Drawdown  \n",
       "Ticker            9434 JT           3864 JT            4506 JT  \n",
       "Value           -1.954547               0.0          -1.628885  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best and worst performing tickers\n",
    "best_total_return = performance_metrics['Total Return'].idxmax()\n",
    "worst_total_return = performance_metrics['Total Return'].idxmin()\n",
    "\n",
    "best_sharpe_ratio = performance_metrics['Sharpe Ratio'].idxmax()\n",
    "worst_sharpe_ratio = performance_metrics['Sharpe Ratio'].idxmin()\n",
    "\n",
    "best_max_drawdown = performance_metrics['Max Drawdown'].idxmax()\n",
    "worst_max_drawdown = performance_metrics['Max Drawdown'].idxmin()\n",
    "\n",
    "best_worst_performance = pd.DataFrame({\n",
    "    'Best Total Return': [best_total_return, performance_metrics.loc[best_total_return, 'Total Return']],\n",
    "    'Worst Total Return': [worst_total_return, performance_metrics.loc[worst_total_return, 'Total Return']],\n",
    "    'Best Sharpe Ratio': [best_sharpe_ratio, performance_metrics.loc[best_sharpe_ratio, 'Sharpe Ratio']],\n",
    "    'Worst Sharpe Ratio': [worst_sharpe_ratio, performance_metrics.loc[worst_sharpe_ratio, 'Sharpe Ratio']],\n",
    "    'Best Max Drawdown': [best_max_drawdown, performance_metrics.loc[best_max_drawdown, 'Max Drawdown']],\n",
    "    'Worst Max Drawdown': [worst_max_drawdown, performance_metrics.loc[worst_max_drawdown, 'Max Drawdown']],\n",
    "}, index=['Ticker', 'Value'])\n",
    "\n",
    "best_worst_performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main drawback of our strategy is that we never use correlation, clustering, or lead-lag effect on our different assets. \n",
    "\n",
    "We also never parametrise our strategy using risk aversion, investment horizon and the quantity of money we want to invest but we have a solid base for a trading strategy.\n",
    "\n",
    "The next step would be first to parametrize each of our univariate signals, find the best parametrisation using out of sample backtesting and than add a new term for each of our asset that is linked to the other to take into account the correlation between assets.\n",
    "Further improvements could include the use of machine learning algorithms or other advanced techniques to optimize the parameters and improve the strategy's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
